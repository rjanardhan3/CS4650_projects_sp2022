\documentclass[11pt, letterpaper]{article}

\makeatletter
\makeatother
\usepackage[hidelinks]{hyperref}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{tabularx}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[margin=0.5in]{geometry}    % For reducing margin
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{physics}
\usepackage{float}
\usepackage{xcolor}
\newcommand{\wx}[1]{\textcolor{magenta}{\bf\small [#1 --WX]}}


\begin{document}
\title{CS 4650: Natural Language Processing \\ Spring 2022 \\ Problem Set 1}
\author{Instructor: Dr. Wei Xu \\ TAs: Chao Jiang, Chase Perry, Rucha Sathe \\Piazza: \url{https://piazza.com/gatech/spring2022/cs4650a}}
\date{Due: Tuesday, Feb 1, 11:59pm ET}
\maketitle

\section{Logistic vs Softmax}

    \begin{enumerate}[label=(\alph*)]
        \item (\textbf{2 pts}) Recall the Logistic and Softmax functions
        
    \begin{align*}
        P_{Logistic}(y=1|\mathbf{x}) = \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}}
    \end{align*}
    
    \begin{align*}
        P_{Softmax}(y|\mathbf{x}) = \frac{e^{\mathbf{w}_y^T\mathbf{x}}}{\sum_{y' \in \mathcal{Y}} e^{\mathbf{w}_{y'}^T\mathbf{x}}} \\
    \end{align*}
    
    Given $\mathcal{Y} = \{0,1\}$, what should be the value of $\mathbf{w}$ such that \\ $P_{Logistic}(y|\textbf{x}) = P_{Softmax}(y|\textbf{x}) \hspace{0.5em} \forall \hspace{0.5em} y \in \mathcal{Y}$? Show your work. \\
    \textit{Hint: Expand the summation term and think about $\mathbf{w}$ in terms of $\mathbf{w}_0$ and $\mathbf{w}_1$.}
    
    
    \item (\textbf{2 pts}) Recall that the Softmax function is a generalization of the logistic sigmoid for multiclass classification. In practice, machine learning software such as PyTorch uses a Softmax implementation for both binary and multiclass classification. Recall that the Softmax function produces a vector output $\mathbf{z} \in \mathbb{R}^{|\mathcal{Y}|}$ and the logistic function a single scalar value $z$, representing class probabilities. Write the equation for a decision rule to produce $\hat{y}$ from the Softmax function in the binary case (when $\mathcal{Y} = \{0,1\}$; you can break ties arbitrarily).
    Write the decision rule to produce $\hat{y}$ from the logistic function. Compare the two rules. How are they similar and/or different? (1-2 sentences).
    \end{enumerate}
    
\newpage

\section{Multiclass Naive Bayes with Bag of Words}
    
    Dr. Smith's clinic has recently begun to provide a preliminary analysis of whether or not a patient is affected by Virus X, based on the description of the patient's condition, uploaded online. The resulting variable can take 3 values, namely: Affected, Unaffected, No Diagnosis. The description of the patient's condition is filtered out on the basis of a select few symptoms, to determine whether or not the patient might be affected. Collected data is displayed in the table below. Dr. Smith's team wishes to put Naive Bayes algorithm to use to carry out the task at hand.
    
    \begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l | r | r | r | r | r | r | r | r | l|}
    \hline
    S.No. & body-ache & dehydrated & headache & cold & nauseous & fever & energetic & hungry & Y \\ \hline
    1           & 0           & 0         & 1    & 1     & 0       & 0       & 0            & 0      & Unaffected \\
    2           & 1           & 0         & 0    & 1     & 0       & 1       & 1            & 1      & No Diagnosis\\
    3           & 0           & 0         & 1    & 1     & 0       & 1       & 1            & 1      & Affected\\
    4           & 0           & 1         & 1    & 1     & 1       & 1       & 0            & 0      & No Diagnosis\\
    5           & 0           & 1         & 0    & 1     & 1       & 0       & 1            & 0      & Unaffected\\
    6           & 0           & 1         & 1    & 1     & 0       & 0       & 1            & 1      & Affected\\
    7           & 0           & 0         & 0    & 1     & 0       & 0       & 0            & 1      & Unaffected\\
    8           & 1           & 0         & 0    & 0     & 1       & 0       & 0            & 0      & Unaffected\\
    \hline
    \end{tabular}
    \end{table}
    
    \begin{enumerate}[label=(\alph*)]
        \item (\textbf{1 pt}) What is the probability $\theta_y$ of each label $y \in$ \{Unaffected, No Diagnosis, Affected\}?\\
        
        \item (\textbf{3 pts}) The parameter $\phi_{y, j}$ is the probability of a token $j$ appearing with label $y$. It is defined by the following equation, where $V$ is the size of the vocabulary set:
        $$ \phi_{y, j} = \frac{\text{count}(y, j)}{\sum^V_{j'=1}\text{count}(y, j')}$$
        
        The probability of a count of words $x$ and a label $y$ is defined as follows:
        $$ \mathrm{p}(x, y; \theta, \phi) = \mathrm{p}(y; \theta) \cdot \mathrm{p}(x|y; \phi) = \mathrm{p}(y; \theta)\prod^V_{j=1} \phi^{x_j}_{y, j} $$
        
        Find the most likely label $\hat y$ for the following word counts vector $x = (0,1,0,1,1,0,0,1)$ using -
        $$\hat y = \text{argmax}_y \log \mathrm{p}(x, y; \theta; \phi).$$
        Show final log (base-10) probabilities for each label rounded to 3 decimals. Treat $\log(0)$ as $-\infty$.
        
        \item (\textbf{3 pts}) When calculating $\text{argmax}_y$, if $\phi_{y, j} = 0$ for a label-word pair, the label $y$  is no longer considered. This is an issue, especially for smaller datasets where a feature may not be present in all documents for a certain label. One approach to mitigating this high variance is to smooth the probabilities. Using add-1 smoothing, which redefines $\phi_{y, j}$, again find the most likely label $\hat y$ for the following word counts vector $x = (0,1,0,1,1,0,0,1)$ using $\hat y = \text{argmax}_y \log \mathrm{p}(x, y; \theta; \phi)$. Make sure to show final log probabilities.\\
        $$\text{add-1 smoothing: } \phi_{y, j} = \frac{1 + \text{count}(y, j)}{V + \sum^V_{j'=1}\text{count}(y, j')}$$\\
    \end{enumerate}
    
\newpage

\section{Perceptron: Linear Separability and Weight Scaling}
\begin{enumerate}[label=(\alph*)]
\item (\textbf{2 pts}) Suppose we have the following data representing the XOR function:

    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline    
         $x_1$ & $x_2$ & $f(x_1,x_2)$ \\
        \hline
        0    & 0 & -1 \\
        0    & 1 & 1 \\
        1    & 0 & 1 \\
        1    & 1 & -1 \\        
        \hline
    \end{tabular}
    \caption{XOR function data}
    \label{tab:my_label}
    \end{table}
    
Evidently, the data is not linearly separable. Therefore the perceptron algorithm will not be able to learn a classifier for XOR, based on this data.

However, we can add a $3^{rd}$ dimension/feature to each input such that the data becomes linearly separable. If we add $(1, 0, 0, 1)$ to the $3^{rd}$ dimension ($x_3$) of the four data points in order, will the new data be linearly separable? Assume 0 is the threshold for classification. Justify your answer.

After the addition of the third dimension, can we say that the perceptron algorithm is actually capable of learning the XOR function? Why or why not?

\bigskip

\item (\textbf{2 pts}) Suppose we have a trained Perceptron with parameters $(W, b)$. If we scale $W$ by a positive constant factor $c$, will the new set of weights produce the exact same prediction for all the test data? Assume the threshold for classification is 0. Justify your answer.

\bigskip

\item (\textbf{2 pts}) With the same setting as 2, this time we translate $W$ by a positive constant factor $c$ (add $c$ to each element of $W$), will the new set of weights produce the exact same prediction for all the test data? Justify your answer.\\
\bigskip

\end{enumerate}

\section{Feedforward Neural Network}
    
    (\textbf{2 pts}) In Question 3, we tried to design a perceptron architecture in order to learn the XOR function represented by Table \ref{tab:my_label}. Now, we want you to design a feedforward neural network to compute the XOR function.\\
    
    \noindent Use a single output node and \textbf{specify} the activation function you choose for it. Also use a single hidden layer with ReLU activation function.  Describe all weights and offsets (bias terms).\\
    
    \noindent \textit{(Hint: In class, we discussed a neural network design that solves the XOR problem using \textbf{tanh} activation functions.)}

\newpage
    \section{Dead Neurons}
        
    The ReLU activation function can lead to ``dead neurons'', which can never be activated on any input. Consider a feedforward neural network with a single hidden layer and ReLU nonlinearity, assuming a binary input vector, $\mathbf{x} \in f\{0,1\}^D$  and scalar output $y$:
    \begin{center}
    $z_i = \text{ReLU}(\mathbf{\theta}_i^{(x \rightarrow z)} \cdot \mathbf{x} + b_i)$ \\
    $\mathbf{y} = \mathbf{\theta}^{(z \rightarrow y)} \cdot \mathbf{z}$
    \end{center}
    
    \noindent Assume the above function is optimized to minimize a loss function (e.g., mean squared error) using stochastic gradient descent. 
    
    \begin{enumerate}[label=(\alph*)]
        \item (\textbf{2 pts}) Under what condition is node $z_i$ ``dead''? Your answer should be expressed in terms of the parameters $\mathbf{\theta}_i^{(x \rightarrow z)}$ and $b_i$.
        
        \item (\textbf{2 pts}) Suppose that the gradient of the loss on a given instance is $\frac{\partial l}{\partial y} = 1$. Derive gradients $\frac{\partial l}{\partial b_i}$ and $\frac{\partial l}{\partial \mathbf{\theta}_{j,i}^{(x \rightarrow z)}}$ for such an instance.
        
        \item (\textbf{2 pts}) Using your answers to the previous two parts, explain why a ``dead'' neuron can never be brought back to life during gradient-based learning. 
        
    \end{enumerate}
    \textit{(Hint: The notation used for this question is in line with that used in Eisenstein Chapter 3.)}
    

\end{document}
